

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Optimization Algorithms (Opti) &mdash; GlobalBioIm Library 1.1.2 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="List of Methods" href="methodssummary.html" />
    <link rel="prev" title="Cost Functions (Cost)" href="cost.html" /> 
</head>

<body class="wy-body-for-nav" style="text-align:justify">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> GlobalBioIm Library
          

          
          </a>

          
            
            
              <div class="version">
                1.1.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Biomedical-Imaging-Group/GlobalBioIm">Download or Clone (v 1.1.2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="infos.html">Important Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="gui.html">Graphical User Interface (GUI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="relatedPapers.html">Related Papers</a></li>
<li class="toctree-l1"><a class="reference internal" href="conditionsuse.html">Conditions of Use</a></li>
</ul>
<p class="caption"><span class="caption-text">Technical Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="abstract.html">Abstract Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="linop.html">Linear Operators (LinOp)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nonlinop.html">Non-Linear Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="cost.html">Cost Functions (Cost)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimization Algorithms (Opti)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#optiadmm">OptiADMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optichambpock">OptiChambPock</a></li>
<li class="toctree-l2"><a class="reference internal" href="#opticonjgrad">OptiConjGrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optidouglasrachford">OptiDouglasRachford</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optifbs">OptiFBS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optifgp">OptiFGP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optigraddsct">OptiGradDsct</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optiprimaldualcondat">OptiPrimalDualCondat</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optirichlucy">OptiRichLucy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optivmlmb">OptiVMLMB</a></li>
<li class="toctree-l2"><a class="reference internal" href="#outputopti">OutputOpti</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#outputopti-default">OutputOpti (Default)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#outputopticonjgrad">OutputOptiConjGrad</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#testcvg">TestCvg</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#testcvg-default">TestCvg (Default)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testcvgcombine">TestCvgCombine</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testcvgcostabsolute">TestCvgCostAbsolute</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testcvgcostrelative">TestCvgCostRelative</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testcvgsteprelative">TestCvgStepRelative</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testcvgmaxsnr">TestCvgMaxSnr</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testcvgadmm">TestCvgADMM</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="methodssummary.html">List of Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="propertiessummary.html">List of Properties</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">Speedup with GPU</a></li>
</ul>
<p class="caption"><span class="caption-text">Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="http://bigwww.epfl.ch">Biomedical Imaging Group</a></li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GlobalBioIm Library</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Optimization Algorithms (Opti)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/opti.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="optimization-algorithms-opti">
<h1>Optimization Algorithms (Opti)<a class="headerlink" href="#optimization-algorithms-opti" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div><p>This section contains optimization algorithms classes which all derive from the abstract class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</div></blockquote>
<span class="target" id="module-Opti"></span><div class="section" id="optiadmm">
<h2>OptiADMM<a class="headerlink" href="#optiadmm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiADMM">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiADMM</code><span class="sig-paren">(</span><em class="sig-param">F0</em>, <em class="sig-param">Fn</em>, <em class="sig-param">Hn</em>, <em class="sig-param">rho_n</em>, <em class="sig-param">solver</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiADMM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Alternating Direction Method of Multipliers [1] algorithm which minimizes <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> of the form
$$ C(\mathrm{x}) = F_0(\mathrm{x}) + \sum_{n=1}^N F_n(\mathrm{H_n x}) $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F_0</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> object</p></li>
<li><p><strong>F_n</strong> – cell of N <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code> for each one</p></li>
<li><p><strong>H_n</strong> – cell of N <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code></p></li>
<li><p><strong>rho_n</strong> – array of N positive scalars</p></li>
<li><p><strong>maxiterCG</strong> – maximal number of inner conjugate gradient (CG) iterations (when required, default 20)</p></li>
<li><p><strong>solver</strong> – a handle function taking parameters solver(z_n,rho_n,x0) (see the note below)</p></li>
<li><p><strong>OutOpCG</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> object for CG (when used)</p></li>
<li><p><strong>ItUpOutCG</strong> – <code class="xref py py-attr docutils literal notranslate"><span class="pre">ItUpOut</span></code> parameter for CG (when used, default 0)</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Principle</strong>
The algorithm aims at minimizing the Lagrangian formulation of the above problem:
$$ \mathcal{L}(\mathrm{x,y_1…y_n,w_1…w_n}) = F_0(\mathrm{x}) + \sum_{n=1}^N \frac12\rho_n\Vert \mathrm{H_nx - y_n + w_n/\rho_n} \Vert^2 + F_n(\mathrm{y_n})$$
using an alternating minimization scheme [1].</p>
<p><strong>Note</strong> The minimization of \(\mathcal{L}\) over \(\mathrm{x}\),
$$ F_0(\mathrm{x}) + \sum_{n=1}^N \frac12\rho_n\Vert \mathrm{H_nx -z_n}\Vert^2, \quad \mathrm{z_n= y_n - w_n/\rho_n} $$
is performed  either using the conjugate-gradient <code class="xref py py-class docutils literal notranslate"><span class="pre">OptiConjGrad</span></code> algorithm, a direct inversion or the given solver</p>
<blockquote>
<div><ul class="simple">
<li><p>If \(F_0\) is empty or is a <code class="xref py py-class docutils literal notranslate"><span class="pre">CostL2</span></code>, then if the <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code> \(\sum_{n=0}^N \mathrm{H_n}^*\mathrm{H_n}\)
is not invertible the <code class="xref py py-class docutils literal notranslate"><span class="pre">OptiConjGrad</span></code> is used by default if no more efficient solver is provided.
Here \(\mathrm{H_0}\) is the <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code> associated to \(F_0\).</p></li>
<li><p>Otherwise the solver is required.</p></li>
</ul>
</div></blockquote>
<p><strong>Reference</strong></p>
<p>[1] Boyd, Stephen, et al. “Distributed optimization and statistical learning via the alternating direction
method of multipliers.” Foundations and Trends in Machine Learning, 2011.</p>
<p><strong>Example</strong> ADMM=OptiADMM(F0,Fn,Hn,rho_n,solver)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OptiConjGrad</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiADMM.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiADMM.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiADMM.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiADMM.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1].</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optichambpock">
<h2>OptiChambPock<a class="headerlink" href="#optichambpock" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiChambPock">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiChambPock</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">H</em>, <em class="sig-param">G</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiChambPock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Chambolle-Pock optimization algorithm [1] which minimizes <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> of the form
$$ C(\mathrm{x}) = F(\mathrm{Hx}) + G(\mathrm{x}) $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code>.</p></li>
<li><p><strong>G</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code>.</p></li>
<li><p><strong>H</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code>.</p></li>
<li><p><strong>tau</strong> – parameter of the algorithm (default 1)</p></li>
<li><p><strong>sig</strong> – parameter of the algorithm which is computed automatically if H.norm is different from -1.</p></li>
<li><p><strong>var</strong> – <p>select the “bar” variable of the algorithm (see [1]):</p>
<ul>
<li><p>if 1 (default) then the primal variable \(\bar{\mathrm{x}} = 2\mathrm{x}_n  - \mathrm{x}_{n-1}\) is used</p></li>
<li><p>if 2 then the dual variable \(\bar{\mathrm{y}} = 2\mathrm{y}_n  - \mathrm{y}_{n-1} \) is used</p></li>
</ul>
</p></li>
<li><p><strong>gam</strong> – <p>if non-empty, accelerated version (see [1]). Here, \(G\) or \(F^*\) is uniformly convex with \(\nabla G^*\) or \(\nabla F\) 1/gam-Lipschitz:</p>
<ul>
<li><p>If \(G\) is uniformly convex then set the parameter var to 1.</p></li>
<li><p>If \(F^*\) is uniformly convex then set the parameter var to 2</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Note-1</strong>: In fact, \(F\) needs only the prox of its fenchel transform <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox_fench()</span></code> (which is implemented 
as soon as $F$ has an implementation of the prox, see <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code>).</p>
<p><strong>Note-2</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>To ensure convergence (see [1]), parameters sig and tau have to verify 
$$ \sigma \times \tau \times \Vert \mathrm{H} \Vert^2 &lt; 1 $$ 
where \(\Vert \mathrm{H}\Vert\) denotes the norm of the linear operator H.</p></li>
<li><p>When the accelerated version is used (i.e. parameter gam is non-empty), 
sig and tau will be updated at each iteration and the initial 
ones (given by user)  have to verify
$$ \sigma \times \tau \times \Vert \mathrm{H} \Vert^2 \leq 1 $$</p></li>
</ul>
</div></blockquote>
<p><strong>Reference</strong>:</p>
<p>[1] Chambolle, Antonin, and Thomas Pock. “A first-order primal-dual algorithm for convex problems with 
applications to imaging.” Journal of Mathematical Imaging and Vision 40.1, pp 120-145 (2011).</p>
<p><strong>Example</strong> CP=OptiChambPock(F,H,G)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiChambPock.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiChambPock.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiChambPock.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiChambPock.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1].</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="opticonjgrad">
<h2>OptiConjGrad<a class="headerlink" href="#opticonjgrad" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiConjGrad">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiConjGrad</code><span class="sig-paren">(</span><em class="sig-param">A</em>, <em class="sig-param">b</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiConjGrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Conjugate gradient optimization algorithm which solves the linear
system \(\mathrm{Ax=b}\) by minimizing
$$ C(\mathrm{x})= \frac12 \mathrm{x^TAx - b^Tx} $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>A</strong> – symmetric definite positive <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code></p></li>
<li><p><strong>b</strong> – right-hand term</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Example</strong> CG=OptiConjGrad(A,b,OutOp)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiConjGrad.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiConjGrad.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiConjGrad.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiConjGrad.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For a detailled
algorithm scheme see <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm">here</a></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optidouglasrachford">
<h2>OptiDouglasRachford<a class="headerlink" href="#optidouglasrachford" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiDouglasRachford">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiDouglasRachford</code><span class="sig-paren">(</span><em class="sig-param">F1</em>, <em class="sig-param">F2</em>, <em class="sig-param">L</em>, <em class="sig-param">gamma</em>, <em class="sig-param">lambda</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiDouglasRachford" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Douglas Rachford splitting optimization algorithm which minimizes
<code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> of the form
$$ C(\mathrm{x}) = F_1(\mathrm{x}) + F_2(\mathrm{L x}) $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F_1</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code>.</p></li>
<li><p><strong>F_2</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code>.</p></li>
<li><p><strong>L</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code> such that \(\mathrm{LL^T} = \nu \mathrm{I} \)</p></li>
<li><p><strong>gamma</strong> – \(\in [0,+\inf[\)</p></li>
<li><p><strong>lambda</strong> – \(\in ]0,2[\) the relaxation parmeter</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Example</strong> DR=OptiDouglasRachford(F1, F2, L, gamma, lambda)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiDouglasRachford.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiDouglasRachford.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiDouglasRachford.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiDouglasRachford.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optifbs">
<h2>OptiFBS<a class="headerlink" href="#optifbs" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiFBS">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiFBS</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">G</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFBS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Forward-Backward Splitting optimization algorithm [1] which minimizes <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> of the form
$$ C(\mathrm{x}) = F(\mathrm{x}) + G(\mathrm{x}) $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F</strong> – a differentiable <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> (i.e. with an implementation of <code class="xref py py-meth docutils literal notranslate"><span class="pre">applyGrad()</span></code>).</p></li>
<li><p><strong>G</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">applyProx()</span></code>.</p></li>
<li><p><strong>gam</strong> – descent step</p></li>
<li><p><strong>fista</strong> – boolean true if the accelerated version FISTA [3] is used (default false)</p></li>
<li><p><strong>momRestart</strong> – boolean true if the moment restart strategy is used [4](default false)</p></li>
<li><p><strong>updateGam</strong> – Rule for updating gamma (none : default, reduced : the parameter gam is decreased according to \(\gamma / \sqrt{k} \), backtracking : backtracking rule following [3])</p></li>
<li><p><strong>eta</strong> – parameter greater than 1 that is used with backtracking (see [3])</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Note</strong>: When the functional are convex and \(F\) has a Lipschitz continuous gradient, convergence is
ensured by taking \(\gamma \in (0,2/L] \) where \(L\) is the Lipschitz constant of \(\nabla F\) (see [1]).
When FISTA is used [3], \(\gamma \) should be in \((0,1/L]\). For nonconvex functions [2] take \(\gamma \in (0,1/L]\).
If \(L\) is known (i.e. F.lip different from -1), parameter \(\gamma\) is automatically set to \(1/L\).</p>
<p><strong>References</strong>:</p>
<p>[1] P.L. Combettes and V.R. Wajs, “Signal recovery by proximal forward-backward splitting”, SIAM Journal on
Multiscale Modeling &amp; Simulation, vol 4, no. 4, pp 1168-1200, (2005).</p>
<p>[2] Hedy Attouch, Jerome Bolte and Benar Fux Svaiter “Convergence of descent methods for semi-algebraic and
tame problems: proximal algorithms, forward-backward splitting, and regularized gaussiedel methods.”
Mathematical Programming, 137 (2013).</p>
<p>[3] Amir Beck and Marc Teboulle, “A Fast Iterative Shrinkage-Thresholding Algorithm for Linear inverse Problems”,
SIAM Journal on Imaging Science, vol 2, no. 1, pp 182-202 (2009)</p>
<p>[4] Brendan O’donoghue and Emmanuel Candès. 2015. Adaptive Restart for Accelerated Gradient Schemes. 
Found. Comput. Math. 15, 3 (June 2015), 715-732.</p>
<p><strong>Example</strong> FBS=OptiFBS(F,G)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiFBS.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFBS.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiFBS.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFBS.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1-3].</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optifgp">
<h2>OptiFGP<a class="headerlink" href="#optifgp" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiFGP">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiFGP</code><span class="sig-paren">(</span><em class="sig-param">F0</em>, <em class="sig-param">TV</em>, <em class="sig-param">bounds</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFGP" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Fast Gradient Proximal which computes the TV proximity operator which
minimizes <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> of the form
$$ C(\mathrm{x}) = \frac12\|\mathrm{x} - \mathrm{y}\|^2_2 + \lambda \|\mathrm{x} \|_{TV} $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F_0</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">CostL2</span></code> object</p></li>
<li><p><strong>TV</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">CostTV</span></code></p></li>
<li><p><strong>bounds</strong> – bounds for set constraint</p></li>
<li><p><strong>gam</strong> – descent step (default 1/8)</p></li>
<li><p><strong>lambda</strong> – regularization parameter for TV</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>References</strong></p>
<p>[1] Beck, A., and Teboulle, M. (2009). Fast gradient-based algorithms for constrained total variation image denoising 
and deblurring problems. IEEE Transactions on Image Processing, 18(11), 2419-2434.</p>
<p><strong>Example</strong> FGP=OptiFGP(F0,TV,bounds)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiFGP.setLambda">
<code class="sig-name descname">setLambda</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">new_l</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFGP.setLambda" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the regularization parameter lambda</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiFGP.setBounds">
<code class="sig-name descname">setBounds</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">new_b</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFGP.setBounds" title="Permalink to this definition">¶</a></dt>
<dd><p>Set constraints bounds</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiFGP.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFGP.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiFGP.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiFGP.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1].</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optigraddsct">
<h2>OptiGradDsct<a class="headerlink" href="#optigraddsct" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiGradDsct">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiGradDsct</code><span class="sig-paren">(</span><em class="sig-param">F</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiGradDsct" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Gradient Descent optimization algorithm to minimize a differentiable <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> \(C(\mathrm{x})\)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>C</strong> – a differentiable <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> (i.e. with an implementation of <code class="xref py py-meth docutils literal notranslate"><span class="pre">applyGrad()</span></code>).</p></li>
<li><p><strong>gam</strong> – descent step</p></li>
<li><p><strong>nagd</strong> – boolean (default false) to activate the Nesterov accelerated gradient descent</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Note</strong> If the cost \(C\) is gradient Lipschitz, convergence is ensured by taking 
\(\gamma \in (0,2/L] \) where \(L\) is the Lipschitz constant of \(\nabla C\) (see [1]).
The optimal choice is \(\gamma = 1/L \) (see [1]). If \(L\) is known (i.e. F.lip different from -1), 
parameter \(\gamma\) is automatically set to \(1/L\).</p>
<p><strong>Reference</strong></p>
<p>[1] Nesterov, Yurii. “Introductory lectures on convex programming.” Lecture Notes (1998): 119-120.</p>
<p><strong>Example</strong> GD=OptiGradDsct(F)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiGradDsct.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiGradDsct.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiGradDsct.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiGradDsct.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.  Performs:
$$ \mathrm{x}^{k+1} = \mathrm{x}^k - \gamma \nabla C(\mathrm{x}^k) $$</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optiprimaldualcondat">
<h2>OptiPrimalDualCondat<a class="headerlink" href="#optiprimaldualcondat" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiPrimalDualCondat">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiPrimalDualCondat</code><span class="sig-paren">(</span><em class="sig-param">F0</em>, <em class="sig-param">G</em>, <em class="sig-param">Fn</em>, <em class="sig-param">Hn</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiPrimalDualCondat" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Primal-Dual algorithm proposed by L. Condat in [1] which minimizes <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> of the form
$$ C(\mathrm{x})= F_0(\mathrm{x}) + G(\mathrm{x}) + \sum_n F_n(\mathrm{H_nx}) $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F_0</strong> – a differentiable <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> (i.e. with an implementation of <code class="xref py py-meth docutils literal notranslate"><span class="pre">grad()</span></code>).</p></li>
<li><p><strong>G</strong> – a <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code>.</p></li>
<li><p><strong>F_n</strong> – cell of N <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code> with an implementation of the <code class="xref py py-meth docutils literal notranslate"><span class="pre">prox()</span></code> for each one</p></li>
<li><p><strong>H_n</strong> – cell of N <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code></p></li>
<li><p><strong>tau</strong> – parameter of the algorithm (see the note below)</p></li>
<li><p><strong>sig</strong> – parameter of the algorithm (see the note below)</p></li>
<li><p><strong>rho</strong> – parameter of the algorithm (see the note below)</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Note</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>When \(F_0=0\), parameters sig and tau have to verify
$$ \sigma \times \tau \Vert \sum_n \mathrm{H_n^*H_n} \Vert \leq 1 $$
and \(\rho \in ]0,2[\), to ensure convergence (see [1, Theorem 5.3]).</p></li>
<li><p>Otherwise, when \(F_0\neq 0\), parameters sig and tau have to verify
$$ \frac{1}{\tau} - \sigma \times \Vert \sum_n \mathrm{H_n^*H_n} \Vert \geq \frac{\beta}{2} $$
where \(\beta\) is the Lipschitz constant of \(\nabla F\) and we need \(\rho \in ]0,\delta[ \) with
$$ \delta = 2 - \frac{\beta}{2}\times\left(\frac{1}{\tau}
- \sigma \times \Vert \sum_n \mathrm{H_n^*H_n}  \Vert\right)^{-1} \in [1,2[ $$
to ensure convergence (see [1, Theorem 5.1]).</p></li>
</ul>
</div></blockquote>
<p><strong>Reference</strong></p>
<p>[1] Laurent Condat, “A Primal-Dual Splitting Method for Convex Optimization Involving Lipchitzian, Proximable and Linear
Composite Terms”, Journal of Optimization Theory and Applications, vol 158, no 2, pp 460-479 (2013).</p>
<p><strong>Example</strong> A=OptiPrimalDualCondat(F0,G,Fn,Hn)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiPrimalDualCondat.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiPrimalDualCondat.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiPrimalDualCondat.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiPrimalDualCondat.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1].
Update xtilde</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optirichlucy">
<h2>OptiRichLucy<a class="headerlink" href="#optirichlucy" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiRichLucy">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiRichLucy</code><span class="sig-paren">(</span><em class="sig-param">F</em>, <em class="sig-param">TV</em>, <em class="sig-param">lamb</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiRichLucy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Richardson-Lucy algorithm [1,2] which minimizes the KullbackLeibler
divergence <code class="xref py py-class docutils literal notranslate"><span class="pre">CostKullLeib</span></code> (with TV regularization [3]).
$$ C(\mathrm{x})= F(\mathrm{x}) + \lambda \Vert \mathrm{x} \Vert_{\mathrm{TV}} $$</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>F</strong> – <code class="xref py py-class docutils literal notranslate"><span class="pre">CostKullLeib</span></code> object or a <code class="xref py py-class docutils literal notranslate"><span class="pre">CostComposition</span></code>
with a <code class="xref py py-class docutils literal notranslate"><span class="pre">CostKullLeib</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">LinOp</span></code></p></li>
<li><p><strong>TV</strong> – boolean true if TV regularization used  (default false)</p></li>
<li><p><strong>lambda</strong> – regularization parameter (when TV used)</p></li>
<li><p><strong>epsl</strong> – smoothing parameter to make TV differentiable at 0 (default \(10^{-6}\))</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Note</strong> An interesting property of this algorithm is that it ensures 
the positivity of the solution from any positive initialization.
However, when TV is used, the positivity of the iterates is not ensured 
anymore if \(\lambda \) is too large. Hence, \(\lambda \) needs to be carefully chosen.</p>
<p><strong>References</strong></p>
<p>[1] Lucy, Leon B. “An iterative technique for the rectification of observed distributions” The astronomical journal (1974)</p>
<p>[2] Richardson, William Hadley. “Bayesian-based iterative method of image restoration.” JOSA (1972): 55-59.</p>
<p>[3] N. Dey et al. “Richardson-Lucy Algorithm With Total Variation Regularization for 3D Confocal Microscope 
Deconvolution.” Microscopy research and technique (2006).</p>
<p><strong>Example</strong> RL=OptiRichLucy(F,TV,lamb)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">CostKullLeib</span></code></p>
<dl class="method">
<dt id="Opti.OptiRichLucy.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiRichLucy.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiRichLucy.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiRichLucy.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1-3].</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optivmlmb">
<h2>OptiVMLMB<a class="headerlink" href="#optivmlmb" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="Opti.OptiVMLMB">
<em class="property">class </em><code class="sig-prename descclassname">Opti.</code><code class="sig-name descname">OptiVMLMB</code><span class="sig-paren">(</span><em class="sig-param">C</em>, <em class="sig-param">xmin</em>, <em class="sig-param">xmax</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiVMLMB" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Abstract.Opti</span></code></p>
<p>Variable Metric Limited Memory Bounded (VMLMB) from OptimPackLegacy [1].
This algorithm
minimizes a cost \(C(\mathrm{x})\) which is differentiable with bound
constraints and/or preconditioning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>C</strong> – minimized cost</p></li>
<li><p><strong>xmin</strong> – min bound (optional)</p></li>
<li><p><strong>xmax</strong> – max bound (optional)</p></li>
</ul>
</dd>
</dl>
<p>All attributes of parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> are inherited.</p>
<p><strong>Note</strong>
This Optimizer has many other variables that are set by
default to reasonable values. See the function m_vmlmb_first.m in the
MatlabOptimPack folder for more details.</p>
<p><strong>Reference</strong></p>
<p>[1] Eric Thiebaut, “Optimization issues in blind deconvolution algorithms”,
SPIE Conf. Astronomical Data Analysis II, 4847, 174-183 (2002).
See OptimPackLegacy <a class="reference external" href="https://github.com/emmt/OptimPackLegacy">repository</a>.</p>
<p><strong>Example</strong> VMLMB=OptiVMLMB(C,xmin,xmax)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">OptiConjGrad</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Cost</span></code></p>
<dl class="method">
<dt id="Opti.OptiVMLMB.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">x0</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiVMLMB.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OptiVMLMB.doIteration">
<code class="sig-name descname">doIteration</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OptiVMLMB.doIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplementation from <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>. For details see [1].</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-Opti.OutputOpti"></span></div>
<div class="section" id="outputopti">
<h2>OutputOpti<a class="headerlink" href="#outputopti" title="Permalink to this headline">¶</a></h2>
<div class="section" id="outputopti-default">
<h3>OutputOpti (Default)<a class="headerlink" href="#outputopti-default" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.OutputOpti.OutputOpti">
<em class="property">class </em><code class="sig-prename descclassname">Opti.OutputOpti.</code><code class="sig-name descname">OutputOpti</code><span class="sig-paren">(</span><em class="sig-param">varargin</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OutputOpti.OutputOpti" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">matlab.mixin.Copyable</span></code></p>
<p>OutputOpti class for algorithms displayings and savings</p>
<p>At each <code class="xref py py-attr docutils literal notranslate"><span class="pre">ItUpOut</span></code> iterations of an optimization algorithm (see <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> generic class),
the update method of an <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code> object will be executed in order to acheive user 
defined computations, e.g.,</p>
<blockquote>
<div><ul class="simple">
<li><p>compute cost / SNR</p></li>
<li><p>store current iterate / cost value</p></li>
<li><p>plot/display stuffs</p></li>
</ul>
</div></blockquote>
<p>The present generic class implements a basic update method that:</p>
<blockquote>
<div><ul class="simple">
<li><p>display the iteration number</p></li>
<li><p>computes &amp; display the cost (if activated)</p></li>
<li><p>computes &amp; display the SNR if ground truth is provided</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – name of the <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code></p></li>
<li><p><strong>computecost</strong> – boolean, if true the cost function will be computed</p></li>
<li><p><strong>evolcost</strong> – array to save the evolution of the cost function</p></li>
<li><p><strong>saveXopt</strong> – boolean (default false) to save the evolution of the optimized variable xopt.</p></li>
<li><p><strong>evolxopt</strong> – cell saving the optimization variable xopt</p></li>
<li><p><strong>iterVerb</strong> – message will be displayed every iterVerb iterations (must be a multiple of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ItUpOut</span></code> parameter of classes <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>)</p></li>
<li><p><strong>costIndex</strong> – select a specific cost function among a sum in the case where the optimized cost function is a sum of cost functions</p></li>
</ul>
</dd>
</dl>
<p><strong>Example</strong> OutOpti=OutputOpti(computecost,iterVerb,costIndex)</p>
<p><strong>Important</strong> The update method should have an unique imput that is the <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> object in order to 
be generic for all Optimization routines. Hence the update method has access (in reading mode) 
to all the properties of <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> objects.</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code></p>
<dl class="method">
<dt id="Opti.OutputOpti.OutputOpti.init">
<code class="sig-name descname">init</code><span class="sig-paren">(</span><em class="sig-param">this</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OutputOpti.OutputOpti.init" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the arrays and counters.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OutputOpti.OutputOpti.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OutputOpti.OutputOpti.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes SNR, cost and display evolution.</p>
</dd></dl>

<dl class="method">
<dt id="Opti.OutputOpti.OutputOpti.computeCost">
<code class="sig-name descname">computeCost</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OutputOpti.OutputOpti.computeCost" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the cost function at the current iterate xopt of
the given <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> opti object</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="outputopticonjgrad">
<h3>OutputOptiConjGrad<a class="headerlink" href="#outputopticonjgrad" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.OutputOpti.OutputOptiConjGrad">
<em class="property">class </em><code class="sig-prename descclassname">Opti.OutputOpti.</code><code class="sig-name descname">OutputOptiConjGrad</code><span class="sig-paren">(</span><em class="sig-param">computecost</em>, <em class="sig-param">yty</em>, <em class="sig-param">xtrue</em>, <em class="sig-param">iterVerb</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OutputOpti.OutputOptiConjGrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.OutputOpti.OutputOptiSNR</span></code></p>
<p>OutputOptiConjGrad class displayings and savings dedicated to for OptiConjGrad</p>
<p>The conjugate gradient algorithm minimizes the function
$$ C(\mathrm{x})= \frac12 \mathrm{x^TAx - b^Tx} $$
However in many cases, it is often used to minimize:
$$ F(\mathrm{x})= \frac12 \|H x - y\|^2_W $$
by setting:
$$\mathrm{A} = \mathrm{H^T W H} \quad \text{and}\quad \mathrm{b = H^T W y}$$
An OutputOptiConjGrad object compute the cost F instead of the cost C.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>computecost</strong> – boolean, if true the cost function will be computed</p></li>
<li><p><strong>xtrue</strong> – ground truth to compute the error with the solution (if provided)</p></li>
<li><p><strong>iterVerb</strong> – message will be displayed every iterVerb iterations (must be a multiple of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ItUpOut</span></code> parameter of classes <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code>)</p></li>
<li><p><strong>ytWy</strong> – weighted norm of $y$ : $ \mathrm{ytWy} = \mathrm{ y^T\,W\,y}$</p></li>
</ul>
</dd>
</dl>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">OptiConjGrad</span></code> <code class="xref py py-class docutils literal notranslate"><span class="pre">OutputOpti</span></code></p>
<dl class="method">
<dt id="Opti.OutputOpti.OutputOptiConjGrad.computeCost">
<code class="sig-name descname">computeCost</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.OutputOpti.OutputOptiConjGrad.computeCost" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the cost function at the current iterate xopt of
the given <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> opti object</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-Opti.TestCvg"></span></div>
</div>
<div class="section" id="testcvg">
<h2>TestCvg<a class="headerlink" href="#testcvg" title="Permalink to this headline">¶</a></h2>
<div class="section" id="testcvg-default">
<h3>TestCvg (Default)<a class="headerlink" href="#testcvg-default" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvg">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvg</code><a class="headerlink" href="#Opti.TestCvg.TestCvg" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">matlab.mixin.Copyable</span></code></p>
<p>TestCvg class  monitor convergence criterion during optimization</p>
<p>At each iterations of an optimization algorithm (see <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> generic class),
the <code class="xref py py-meth docutils literal notranslate"><span class="pre">testConvergence()</span></code> method of an <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code> object will be executed in order to acheive user
defined computations</p>
<p><strong>Example</strong> CvOpti=TestCvg()</p>
<p><strong>Important</strong> The <code class="xref py py-meth docutils literal notranslate"><span class="pre">testConvergence()</span></code> method should have an unique imput that is the <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> object in order to
be generic for all Optimization routines. Hence the update method has access (in reading mode)
to all the properties of <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code> objects.</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvg.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvg.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Default implementation: do nothing (algorithm will break with
max iterations).</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="testcvgcombine">
<h3>TestCvgCombine<a class="headerlink" href="#testcvgcombine" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvgCombine">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvgCombine</code><span class="sig-paren">(</span><em class="sig-param">varargin</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgCombine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.TestCvg.TestCvg</span></code></p>
<p>TestCvgCombine:
Combine several <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code> objects.</p>
<p><strong>Examples</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>CvOpti = TestCvgCombine(A, B ,C); 
where A B and C are of TestCvg class</p></li>
<li><p>CvOpti = TestCvgCombine(‘CostRelative’,0.000001, ‘CostAbsolute’,10);  
for simple test</p></li>
</ul>
</div></blockquote>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvgCombine.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgCombine.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplemented from parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="testcvgcostabsolute">
<h3>TestCvgCostAbsolute<a class="headerlink" href="#testcvgcostabsolute" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvgCostAbsolute">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvgCostAbsolute</code><span class="sig-paren">(</span><em class="sig-param">costAbsoluteTol</em>, <em class="sig-param">costIndex</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgCostAbsolute" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.TestCvg.TestCvg</span></code></p>
<p>TestCvgCostAbsolute stops the optimization when the cost function is below the value COSTABSOLUTETOL</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>costAbsoluteTol</strong> – absolute tolerance on cost function</p></li>
<li><p><strong>costIndex</strong> – select a specific cost function among a sum in the case where the optimized cost function is a sum of cost functions</p></li>
</ul>
</dd>
</dl>
<p><strong>Example</strong> CvOpti=TestCvgCostAbsolute(costAbsoluteTol, costIndex )</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvgCostAbsolute.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgCostAbsolute.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests algorithm convergence</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>boolean true if \( C(\mathrm{x^k}) &lt; \mathrm{costAbsoluteTol}\)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="testcvgcostrelative">
<h3>TestCvgCostRelative<a class="headerlink" href="#testcvgcostrelative" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvgCostRelative">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvgCostRelative</code><span class="sig-paren">(</span><em class="sig-param">costRelativeTol</em>, <em class="sig-param">costIndex</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgCostRelative" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.TestCvg.TestCvg</span></code></p>
<p>TestCvgCostRelative stops the optimization when the cost function is below the value COSTRELATIVETOL</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>costRelativeTol</strong> – relative tolerance on cost function</p></li>
<li><p><strong>costIndex</strong> – select a specific cost function among a sum in the case where the optimized cost function is a sum of cost functions</p></li>
</ul>
</dd>
</dl>
<p><strong>Example</strong> CvOpti=TestCvgCostRelative(costRelativeTol, costIndex )</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvgCostRelative.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgCostRelative.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests algorithm convergence from the relative difference between two successive value of the cost function</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>boolean true if
$$ \frac{\left| C(\mathrm{x}^{k}) - C(\mathrm{x}^{k-1})\right|}{\left|C(\mathrm{x}^{k-1})\right|} &lt; \mathrm{costRelativeTol}$$</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="testcvgsteprelative">
<h3>TestCvgStepRelative<a class="headerlink" href="#testcvgsteprelative" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvgStepRelative">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvgStepRelative</code><span class="sig-paren">(</span><em class="sig-param">stepRelativeTol</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgStepRelative" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.TestCvg.TestCvg</span></code></p>
<p>TestCvgStepRelative stops the optimization when the step  is below
the value STEPRELATIVETOL</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stepRelativeTol</strong> – relative tolerance on step</p>
</dd>
</dl>
<p><strong>Example</strong> CvOpti=TestCvgStepRelative(stepRelativeTol )</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvgStepRelative.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgStepRelative.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests algorithm convergence from the relative difference between two successive value of the step function</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>boolean true if
$$ \frac{\| \mathrm{x}^{k} - \mathrm{x}^{k-1}\|}{\|\mathrm{x}^{k-1}\|} &lt; \text{stepRelativeTol}.$$</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="testcvgmaxsnr">
<h3>TestCvgMaxSnr<a class="headerlink" href="#testcvgmaxsnr" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvgMaxSnr">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvgMaxSnr</code><span class="sig-paren">(</span><em class="sig-param">ref</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgMaxSnr" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.TestCvg.TestCvg</span></code></p>
<p>TestCvgMaxSnr stops the optimization when the SNR is decreasing</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ref</strong> – reference signal</p>
</dd>
</dl>
<p><strong>Example</strong> CvOpti=TestCvgMaxSnr(ref )</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvgMaxSnr.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgMaxSnr.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests algorithm convergence using the SNR with</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>boolean true if
$$ 20\log\left(\frac{\| \mathrm{ref}\|}{\| \mathrm{x}^{k} - \mathrm{ref}\|}\right)&lt; 20\log\left(\frac{\| \mathrm{ref}\|}{\| \mathrm{x}^{k-1} - \mathrm{ref}\|}\right)$$</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="testcvgadmm">
<h3>TestCvgADMM<a class="headerlink" href="#testcvgadmm" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="Opti.TestCvg.TestCvgADMM">
<em class="property">class </em><code class="sig-prename descclassname">Opti.TestCvg.</code><code class="sig-name descname">TestCvgADMM</code><span class="sig-paren">(</span><em class="sig-param">eps_abs</em>, <em class="sig-param">eps_rel</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgADMM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Opti.TestCvg.TestCvg</span></code></p>
<p>Test convergence by monitoring the primal and dual rediduals in ADMM
as described in [1].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eps_abs</strong> – absolute tolerance (default 0, see [1])</p></li>
<li><p><strong>eps_rel</strong> – relative tolerance (default 1e-3 see [1])</p></li>
</ul>
</dd>
</dl>
<p><strong>Reference</strong></p>
<p>[1] Boyd, Stephen, et al. “Distributed optimization and statistical learning via the alternating direction
method of multipliers.” Foundations and Trends in Machine Learning, 2011.</p>
<p><strong>Warning</strong> the termination criterion described in [1] requires to
apply the adjoint of Hn at every iteration, which may be costly</p>
<p><strong>Example</strong> CvOpti=TestCvgADMM(eps_abs,eps_rel)</p>
<p>See also <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code></p>
<dl class="method">
<dt id="Opti.TestCvg.TestCvgADMM.testConvergence">
<code class="sig-name descname">testConvergence</code><span class="sig-paren">(</span><em class="sig-param">this</em>, <em class="sig-param">opti</em><span class="sig-paren">)</span><a class="headerlink" href="#Opti.TestCvg.TestCvgADMM.testConvergence" title="Permalink to this definition">¶</a></dt>
<dd><p>Reimplemented from parent class <code class="xref py py-class docutils literal notranslate"><span class="pre">TestCvg</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="methodssummary.html" class="btn btn-neutral float-right" title="List of Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="cost.html" class="btn btn-neutral float-left" title="Cost Functions (Cost)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017, Biomedical Imaging Group (EPFL).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>